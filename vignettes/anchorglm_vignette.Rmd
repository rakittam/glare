---
title: "Generalized Linear Anchor Regression"
author: "Maic Rakitta"
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Generalized Linear Anchor Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
This vignette offers an introduction with examples to the first Generalized
Linear Anchor REgression (GLARE) package version. We extended the work of
Dominik Rothenhaeusler, Nicolai Meinshausen, Peter Buehlmann and Jonas Peters,
who developed a linear anchor regression method. GLARE extends their work from a
linear to a generalized linear framework. In this vignette we give only a thin
theoretical introduction to GLARE and focus more on how the reader can apply the
corresponding R package. Since the development is not yet finished, we are
grateful for your input. Furthermore, we cannot yet provide GLARE literature.
For anchor regression we recommend the corresponding work of Rothenhaeusler et
al.

This vignette makes no claim to completeness, but rather provides a first
insight into our method.

## Short Introduction
Similarly to the theory of Rothenhaeusler et al., GLARE estimates its parameter
by minimizing an objective function. However, instead of interpolating between
the solutions of ordinary least squares and two-stage least squares, we adapt
GLARE to the GLM framework. The main differences are that we use log likelihood
instead of OLS and that suitable residuals must be used. The minimization
results in
$$\beta^\xi = \arg\min_\beta -\frac{1}{n}l(\beta; Y, X) +
\frac{1}{n}\xi ||Ar||_2^2,$$
where $X \in \mathbb{R}^{n\times p}$ are the covariates, $Y \in \mathbb{R}^{n}$
is the response, $A \in \mathbb{R}^{n\times q}$ are the anchor variables and
$r \in \mathbb{R}^{n}$ are appropriate residuals (e.g. deviance residuals,
pearson residuals). The hyperparameter $\xi \in \{-1, \infty\}$ interpolates
between maximizing the log-likelihood, $l$, and partialling out the effect of
the anchor onto the residuals. Note that there can also be hidden variables,
$H \in \mathbb{R}^{n\times h}$, influencing both the response and the covariates.

## Getting started
The package is still in development. For the most up-to-date version, feel free
to send a request directly to the author (rakittam@student.ethz.ch) to receive
the corresponding source package.

If you need help to install the package from source, please have a look at
http://www.ryantmoore.org/files/ht/htrtargz.pdf .

For this vignette, up to GLARE, the following libraries are necessary:

```{r}
library(glare) 
library(stats) # used to generate data sets
library(ggplot2) # used for plots

```

## Example: Binomial Framework
The given example is constructed in a binomial framework. First we
construct an artificial data set. Next, we show how to apply GLARE to the
generated data. Finally, we investigate the model on a perturbed data set and
plot some result.

### Data Construction
In this section we simulate the unperturbed data for the binomial example. The chosen model for this example is
$$
\begin{aligned}
H &\sim \mathcal{N}(0, 1)\\
A &\sim \text{Rademacher}_2\\
X &= G A + H + \epsilon_X, \text{ where } \epsilon_X \sim \mathcal{N}_2(0, 1)\\
Y&\sim \text{binomial}(m, p), \text{ where } g(p) = \text{logit}(p) = b^TX + H + GA\text{ .} \end{aligned}
$$

If
you want to reproduce our results, please set the corresponding seed,
`set.seed(1992)`, before you generate the data.

```{r}
set.seed(1992)

# Number of observations from unperturbed distribution
n <- 1000 

# Setting anchor coefficients
g1 <- 0.5
g2 <- -0.2
g3 <- -0.4
g4 <- -2

# Number of trials for binary distribution
m <- sample(1:5, size = n, replace = TRUE) 

# Anchor variables
A <- matrix(sample(c(-1, 1), size = n * 2, replace = TRUE),
            nrow = n, ncol = 2) # rademacher distribution

# Hidden confounders
epsH <- matrix(rnorm(n = n * 1, mean = 0, sd = 1),
               nrow = n, ncol = 1)
H <- epsH 

# Confounders
epsX <- matrix(rnorm(n = n * 2, mean = 0, sd = 1),
               nrow = n, ncol = 2)
X <- matrix(nrow = n, ncol = 2) 
X[, 1] <- g1 * A[, 1] + g2 * A[, 2] + H + epsX[, 1]
X[, 2] <- g1 * A[, 1] + g3 * A[, 2] + H + epsX[, 2]

# Response
p <- binomial()$linkinv(3 * X[, 1] + 3 * X[, 2] + H + g4 * A[, 1])
Y <- matrix(rbinom(n = n, size = m, prob = p),
            nrow = n, ncol = 1)

data <- data.frame(Y = Y, X = X, A = A, m = m)
```

### How to apply `glare`
Now that we have an artificial training data, we can apply GLARE. Response and
covariate variables are fed into the
method using the `formula` class. Also the anchors are provided with a
corresponding `formula`.
Note that we are not using an intercept, and hence added `- 1` to the formula
expression. We encourage the reader to have a look at the help file, `?glare`,
for further information.

For a first intuition we set the hyperparameter, $\xi$, to
two and use deviance residuals for the anchor penalty.

```{r}
aglm_fit <- glare(formula = Y ~ X.1 + X.2 - 1,
                  A_formula = ~ A.1 + A.2 - 1,
                  data = data,
                  xi = 2,
                  family = "binomial",
                  type = "deviance")
```

By specifying an anchor variable, a symbolic description of the linear predictor
and a description of the error distribution, we have fitted a generalized linear
anchor model. The function returns an object of class `"glare"`.

It shall be noted that `glare` can handle different form of input for
binomial data, i.e.
```{r}
# Response as a matrix with number of success and losses for each observation
SL <- cbind(Y, m - Y)
aglm_fit_SL <- glare(formula = SL ~ X.1 + X.2 - 1,
                     A_formula = ~ A.1 + A.2 - 1,
                     data = data,
                     xi = 2,
                     family = "binomial",
                     type = "deviance")
```

### Interpretation on a perturbed data set
In this section we want to investigate our model trained on the initial data
using different values for $\xi$, on a perturbed data set.

#### Perturbed data set
First we simulate a perturbed data set by manipulating the anchor coefficients,
while letting the rest the same as in the initial data.

```{r}
# Initialize perturbed anchor coefficients
g1_pert <- -1.5
g2_pert <- -0.5
g3_pert <- -0.4
g4_pert <- 2

# Number of trials for binary distribution
m_pert <- sample(1:5, size = n, replace = TRUE)

# Anchor variables are drawn from a rademacher distribution
A_pert <- matrix(sample(c(-1, 1), size = n * 2, replace = TRUE),
                 nrow = n, ncol = 2)

# Hidden confounders
epsH_pert <- matrix(rnorm(n = n * 1, mean = 0, sd = 1),
                    nrow = n, ncol = 1)
H_pert <- epsH_pert

# Confounders
epsX_pert <- matrix(rnorm(n = n * 2, mean = 0, sd = 1),
                    nrow = n, ncol = 2)
X_pert <- matrix(nrow = n, ncol = 2)
X_pert[, 1] <- g1_pert * A_pert[, 1] + g2_pert * A_pert[, 2] + H_pert + epsX_pert[, 1]
X_pert[, 2] <- g1_pert * A_pert[, 1] + g3_pert * A_pert[, 2] + H_pert + epsX_pert[, 2]

# Response
p_pert <- binomial()$linkinv(3 * X_pert[, 1] + 3 * X_pert[, 2] + H_pert + g4_pert * A_pert[, 1])
Y_pert <- matrix(rbinom(n = n, size = m_pert, prob = p_pert),
                 nrow = n, ncol = 1)

# Data frame of perturbed data
data_pert <- data.frame(Y = Y_pert, X = X_pert, A = A_pert, m = m_pert)
```

#### Iterating over the hyperparameter
Next, we iterate over $\xi$ and use `glare` for each iteration. We store the
estimated coefficients and the log-likelihood on the perturbed data set.
```{r}
xi_vec <- seq(-1, 100, by = 1)
b_matrix <- matrix(nrow = length(xi_vec), ncol = ncol(X))
loglikelihood_pert <- numeric(length(xi_vec))

for (i in 1:length(xi_vec)) {

  xi <- xi_vec[i]
  fit_temp <- glare(formula = Y ~ X - 1,
                    A_formula = ~ A - 1,
                    data = data,
                    xi = xi,
                    family = "binomial",
                    type = "deviance")

  b_matrix[i, ] <- coef(fit_temp)
  loglikelihood_pert[i] <- logLik(fit_temp, newdata = data_pert)
}
```

Moreover, we calculate additional estimates for comparison: maximum likelihood
estimation, partialling out the effect of the anchors A and
letting $\xi$ go to infinity. In the following the corresponding parameter
estimates and the corresponding log-likelihood on the perturbed data is given.
The reader should be aware that like anchor regression, GLARE does not aim for
the true underlying causal function, but rather tries to maximize the
log-likelihood.
```{r}
# Optimal xi ------------------------------------------------------------------
xi_opt <- xi_vec[which.max(loglikelihood_pert)]
xi_opt
b_opt <- b_matrix[which.max(loglikelihood_pert), ]
b_opt
loglikelihood_pert_opt <- max(loglikelihood_pert)
loglikelihood_pert_opt

# MLE -------------------------------------------------------------------------
xi_MLE <- 0
b_MLE <- b_matrix[which(xi_vec == xi_MLE), ]
b_MLE
# which by construction is the same as
glm(cbind(Y, m - Y) ~ X - 1, family = "binomial")$coef
loglikelihood_pert_MLE <- loglikelihood_pert[which(xi_vec == xi_MLE)]
loglikelihood_pert_MLE 

# PA --------------------------------------------------------------------------
xi_PA <- -1
b_PA <- b_matrix[which(xi_vec == xi_PA), ]
b_PA
loglikelihood_pert_PA <- loglikelihood_pert[which(xi_vec == xi_PA)]
loglikelihood_pert_PA

# big xi ----------------------------------------------------------------------
xi_big <- 10000000
fit_big <- glare(formula = Y ~ X - 1,
                 A_formula = ~ A - 1,
                 data = data,
                 xi = xi_big,
                 family = "binomial",
                 type = "deviance")
b_big <- coef(fit_big)
b_big
loglikelihood_pert_big <- logLik(fit_big, newdata = data_pert)
loglikelihood_pert_big
```

#### Plot of the results
Finally, we can plot the log-likelihood on the perturbed data against the
hyperparameter values. As we can see in the figure, in this setup GLARE
outperforms the other methods in terms of maximal log-likelihood on the
perturbed data.

```{r, fig.height=6, fig.width=6}
ggplot_data <- data.frame(loglikelihood_pert = loglikelihood_pert,
                          xi_vec = xi_vec)
ggplot(data = ggplot_data, aes(y = loglikelihood_pert, x = xi_vec)) +

  geom_line() +
  
  labs(title = "log-likelihood of the perturped data set",
       x = "Hyperparameter xi", y = "log-likelihood") +

  annotate("point", colour = "red",
           x = xi_opt, y = loglikelihood_pert_opt) +
  annotate("text", label = "optimal xi",
           x = xi_opt, y = loglikelihood_pert_opt + 150) +
  annotate("point", colour = "green4",
           x = xi_MLE, y = loglikelihood_pert_MLE) +
  annotate("text", label = "MLE",
           x = xi_MLE + 5, y = loglikelihood_pert_MLE) +
  annotate("point", colour = "blue1",
           x = xi_PA, y = loglikelihood_pert_PA) +
  annotate("text", label = "PA",
           x = xi_PA + 4, y = loglikelihood_pert_PA) +

  geom_hline(yintercept = loglikelihood_pert_big,
             linetype = "dashed", color = "gray") +
  annotate("text", label = "Approximated convergence",
           x = 85, y = loglikelihood_pert_big + 100)
```

### Methods
We provide several generic accessor functions for the `glare` class, i.e.
`logLik`, `coef`, `predict` and `residuals`. For a list of all currently
provided methods
have a look at `methods(class = "glare")`.

For example you can generate predictions given by the learned model for either
the initial or for new data.

```{r, results='hide'}
predict(aglm_fit) # predictions for initial unperturbed data
predict(aglm_fit, newdata = data_pert) # predictions for perturbed data
```
